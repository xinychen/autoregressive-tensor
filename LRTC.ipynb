{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LRMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_mape(var, var_hat):\n",
    "    return np.sum(np.abs(var - var_hat) / var) / var.shape[0]\n",
    "\n",
    "def compute_rmse(var, var_hat):\n",
    "    return np.sqrt(np.sum((var - var_hat) ** 2) / var.shape[0])\n",
    "\n",
    "def svt(mat, tau):\n",
    "    u, s, v = np.linalg.svd(mat, full_matrices = 0)\n",
    "    vec = s - tau\n",
    "    vec[vec < 0] = 0\n",
    "    return u @ np.diag(vec) @ v\n",
    "\n",
    "def LRMC(dense_mat, sparse_mat, rho, maxiter = 100):\n",
    "    \n",
    "    pos_train = np.where(sparse_mat != 0)\n",
    "    mat_train = sparse_mat[pos_train]\n",
    "    pos_test = np.where((sparse_mat == 0) & (dense_mat != 0))\n",
    "    mat_test = dense_mat[pos_test]\n",
    "    \n",
    "    X = sparse_mat.copy()\n",
    "    Z = sparse_mat.copy()\n",
    "    T = sparse_mat.copy()\n",
    "    del dense_mat, sparse_mat\n",
    "    show_iter = 100\n",
    "    for it in range(maxiter):\n",
    "        Z = svt(X + T / rho, 1 / rho)\n",
    "        X = Z - T / rho\n",
    "        X[pos_train] = mat_train\n",
    "        T = T - rho * (Z - X)\n",
    "        if (it + 1) % show_iter == 0:\n",
    "            print(it + 1)\n",
    "            print(compute_mape(mat_test, X[pos_test]))\n",
    "            print(compute_rmse(mat_test, X[pos_test]))\n",
    "            print()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "dense_mat = np.load('pems-w1.npz')['arr_0']\n",
    "for t in range(2, 5):\n",
    "    dense_mat = np.append(dense_mat, np.load('pems-w{}.npz'.format(t))['arr_0'],\n",
    "                          axis = 1)\n",
    "dim1, dim2 = dense_mat.shape\n",
    "\n",
    "missing_rate = 0.9\n",
    "sparse_mat = dense_mat * np.round(np.random.rand(dim1, dim2) + 0.5 - missing_rate)\n",
    "np.savez_compressed('dense_mat.npz', dense_mat)\n",
    "np.savez_compressed('sparse_mat.npz', sparse_mat)\n",
    "\n",
    "import cupy as np\n",
    "\n",
    "dense_mat = np.load('dense_mat.npz')['arr_0'][:, : 14 * 288]\n",
    "sparse_mat = np.load('sparse_mat.npz')['arr_0'][:, : 14 * 288]\n",
    "\n",
    "import time\n",
    "for lmbda in [1e-4, 1e-5]:\n",
    "    print('lmbda = {}'.format(lmbda))\n",
    "    start = time.time()\n",
    "    alpha = np.ones(3) / 3\n",
    "    tensor_hat = LRMC(dense_tensor, sparse_tensor, lmbda)\n",
    "    end = time.time()\n",
    "    print('Running time: %d seconds.'%(end - start))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report imputation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "dense_mat = np.load('pems-w1.npz')['arr_0']\n",
    "for t in range(2, 5):\n",
    "    dense_mat = np.append(dense_mat, np.load('pems-w{}.npz'.format(t))['arr_0'],\n",
    "                          axis = 1)\n",
    "dim1, dim2 = dense_mat.shape\n",
    "\n",
    "missing_rate = 0.9\n",
    "sparse_mat = dense_mat * np.round(np.random.rand(dim1, dim2) + 0.5 - missing_rate)\n",
    "np.savez_compressed('dense_mat.npz', dense_mat)\n",
    "np.savez_compressed('sparse_mat.npz', sparse_mat)\n",
    "\n",
    "import cupy as np\n",
    "\n",
    "dense_mat = np.load('dense_mat.npz')['arr_0'][:, 14 * 288 :]\n",
    "sparse_mat = np.load('sparse_mat.npz')['arr_0'][:, 14 * 288 :]\n",
    "\n",
    "import time\n",
    "for lmbda in [1e-4, 1e-5]:\n",
    "    print('lmbda = {}'.format(lmbda))\n",
    "    start = time.time()\n",
    "    alpha = np.ones(3) / 3\n",
    "    tensor_hat = LRMC(dense_tensor, sparse_tensor, lmbda)\n",
    "    end = time.time()\n",
    "    print('Running time: %d seconds.'%(end - start))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HaLRTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv as inv\n",
    "\n",
    "def ten2mat(tensor, mode):\n",
    "    return np.reshape(np.moveaxis(tensor, mode, 0), (tensor.shape[mode], -1), order = 'F')\n",
    "\n",
    "def mat2ten(mat, tensor_size, mode):\n",
    "    index = list()\n",
    "    index.append(mode)\n",
    "    for i in range(tensor_size.shape[0]):\n",
    "        if i != mode:\n",
    "            index.append(i)\n",
    "    return np.moveaxis(np.reshape(mat, tensor_size[index].tolist(), order = 'F'), 0, mode)\n",
    "\n",
    "def svt_tnn(mat, alpha, rho, theta):\n",
    "    \"\"\"This is a Numpy dependent singular value thresholding (SVT) process.\"\"\"\n",
    "    u, s, v = np.linalg.svd(mat, full_matrices = 0)\n",
    "    vec = s.copy()\n",
    "    vec[theta :] = s[theta :] - alpha / rho\n",
    "    vec[vec < 0] = 0\n",
    "    return u @ np.diag(vec) @ v\n",
    "\n",
    "def compute_rmse(var, var_hat):\n",
    "    return np.sqrt(np.sum((var - var_hat) ** 2) / var.shape[0])\n",
    "\n",
    "def compute_mape(var, var_hat):\n",
    "    return np.sum(np.abs(var - var_hat) / var) / var.shape[0]\n",
    "\n",
    "def LRTC(dense_tensor, sparse_tensor, alpha, rho, theta, \n",
    "         epsilon = 1e-4, maxiter = 100):\n",
    "    \"\"\"Low-Rank Tenor Completion with Truncated Nuclear Norm, LRTC-TNN.\"\"\"\n",
    "    \n",
    "    dim = np.array(sparse_tensor.shape)\n",
    "    pos_missing = np.where(sparse_tensor == 0)\n",
    "    pos_test = np.where((dense_tensor != 0) & (sparse_tensor == 0))\n",
    "    \n",
    "    X = np.zeros((int(len(dim)), int(dim[0]), int(dim[1]), int(dim[2])))\n",
    "    T = np.zeros((int(len(dim)), int(dim[0]), int(dim[1]), int(dim[2]))) \n",
    "    Z = sparse_tensor.copy()\n",
    "    last_tensor = sparse_tensor.copy()\n",
    "    snorm = np.sqrt(np.sum(sparse_tensor ** 2))\n",
    "    it = 0\n",
    "    while True:\n",
    "        rho = min(rho * 1.05, 1e5)\n",
    "        for k in range(len(dim)):\n",
    "            X[k] = mat2ten(svt_tnn(ten2mat(Z - T[k] / rho, k), alpha[k], rho, theta), dim, k)\n",
    "        Z[pos_missing] = np.mean(X + T / rho, axis = 0)[pos_missing]\n",
    "        temp = np.zeros((int(len(dim)), int(dim[0]), int(dim[1]), int(dim[2])))\n",
    "        for i in range(int(len(dim))):\n",
    "            temp[i] = Z\n",
    "        T = T + rho * (X - temp)\n",
    "        tensor_hat = np.einsum('k, kmnt -> mnt', alpha, X)\n",
    "        tol = np.sqrt(np.sum((tensor_hat - last_tensor) ** 2)) / snorm\n",
    "        last_tensor = tensor_hat.copy()\n",
    "        it += 1\n",
    "        if (it + 1) % 100 == 0:\n",
    "            print('Iter: {}'.format(it + 1))\n",
    "            print('MAPE: {:.6}'.format(compute_mape(dense_tensor[pos_test], tensor_hat[pos_test])))\n",
    "            print('RMSE: {:.6}'.format(compute_rmse(dense_tensor[pos_test], tensor_hat[pos_test])))\n",
    "            print()\n",
    "        if (tol < epsilon) or (it >= maxiter):\n",
    "            break\n",
    "\n",
    "    print('Imputation MAPE: {:.6}'.format(compute_mape(dense_tensor[pos_test], tensor_hat[pos_test])))\n",
    "    print('Imputation RMSE: {:.6}'.format(compute_rmse(dense_tensor[pos_test], tensor_hat[pos_test])))\n",
    "    print()\n",
    "    \n",
    "    return tensor_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "dense_mat = np.load('pems-w1.npz')['arr_0']\n",
    "for t in range(2, 5):\n",
    "    dense_mat = np.append(dense_mat, np.load('pems-w{}.npz'.format(t))['arr_0'],\n",
    "                          axis = 1)\n",
    "dim1, dim2 = dense_mat.shape\n",
    "\n",
    "missing_rate = 0.3\n",
    "sparse_mat = dense_mat * np.round(np.random.rand(dim1, dim2) + 0.5 - missing_rate)\n",
    "np.savez_compressed('dense_mat.npz', dense_mat)\n",
    "np.savez_compressed('sparse_mat.npz', sparse_mat)\n",
    "\n",
    "import cupy as np\n",
    "\n",
    "dense_mat = np.load('dense_mat.npz')['arr_0'][:, : 14 * 288]\n",
    "sparse_mat = np.load('sparse_mat.npz')['arr_0'][:, : 14 * 288]\n",
    "dense_tensor = mat2ten(dense_mat, np.array([dim1, 288, 14]), 0)\n",
    "sparse_tensor = mat2ten(sparse_mat, np.array([dim1, 288, 14]), 0)\n",
    "del dense_mat, sparse_mat\n",
    "\n",
    "import time\n",
    "for lmbda in [1e-4, 1e-5]:\n",
    "    for theta in [0]:\n",
    "        print('lmbda = {}'.format(lmbda))\n",
    "        print('theta = {}'.format(theta))\n",
    "        start = time.time()\n",
    "        alpha = np.ones(3) / 3\n",
    "        tensor_hat = LRTC(dense_tensor, sparse_tensor,\n",
    "                          alpha, lmbda, theta)\n",
    "        end = time.time()\n",
    "        print('Running time: %d seconds.'%(end - start))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report imputation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "dense_mat = np.load('pems-w1.npz')['arr_0']\n",
    "for t in range(2, 5):\n",
    "    dense_mat = np.append(dense_mat, np.load('pems-w{}.npz'.format(t))['arr_0'],\n",
    "                          axis = 1)\n",
    "dim1, dim2 = dense_mat.shape\n",
    "\n",
    "missing_rate = 0.3\n",
    "sparse_mat = dense_mat * np.round(np.random.rand(dim1, dim2) + 0.5 - missing_rate)\n",
    "np.savez_compressed('dense_mat.npz', dense_mat)\n",
    "np.savez_compressed('sparse_mat.npz', sparse_mat)\n",
    "\n",
    "import cupy as np\n",
    "\n",
    "dense_mat = np.load('dense_mat.npz')['arr_0'][:, 14 * 288 :]\n",
    "sparse_mat = np.load('sparse_mat.npz')['arr_0'][:, 14 * 288 :]\n",
    "dense_tensor = mat2ten(dense_mat, np.array([dim1, 288, 14]), 0)\n",
    "sparse_tensor = mat2ten(sparse_mat, np.array([dim1, 288, 14]), 0)\n",
    "del dense_mat, sparse_mat\n",
    "\n",
    "import time\n",
    "for lmbda in [1e-5]:\n",
    "    for theta in [0]:\n",
    "        print('lmbda = {}'.format(lmbda))\n",
    "        print('theta = {}'.format(theta))\n",
    "        start = time.time()\n",
    "        alpha = np.ones(3) / 3\n",
    "        tensor_hat = LRTC(dense_tensor, sparse_tensor,\n",
    "                          alpha, lmbda, theta)\n",
    "        end = time.time()\n",
    "        print('Running time: %d seconds.'%(end - start))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LRTC-TNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv as inv\n",
    "\n",
    "def ten2mat(tensor, mode):\n",
    "    return np.reshape(np.moveaxis(tensor, mode, 0), (tensor.shape[mode], -1), order = 'F')\n",
    "\n",
    "def mat2ten(mat, tensor_size, mode):\n",
    "    index = list()\n",
    "    index.append(mode)\n",
    "    for i in range(tensor_size.shape[0]):\n",
    "        if i != mode:\n",
    "            index.append(i)\n",
    "    return np.moveaxis(np.reshape(mat, tensor_size[index].tolist(), order = 'F'), 0, mode)\n",
    "\n",
    "def svt_tnn(mat, alpha, rho, theta):\n",
    "    \"\"\"This is a Numpy dependent singular value thresholding (SVT) process.\"\"\"\n",
    "    u, s, v = np.linalg.svd(mat, full_matrices = 0)\n",
    "    vec = s.copy()\n",
    "    vec[theta :] = s[theta :] - alpha / rho\n",
    "    vec[vec < 0] = 0\n",
    "    return u @ np.diag(vec) @ v\n",
    "\n",
    "def compute_rmse(var, var_hat):\n",
    "    return np.sqrt(np.sum((var - var_hat) ** 2) / var.shape[0])\n",
    "\n",
    "def compute_mape(var, var_hat):\n",
    "    return np.sum(np.abs(var - var_hat) / var) / var.shape[0]\n",
    "\n",
    "def LRTC(dense_tensor, sparse_tensor, alpha, rho, theta, \n",
    "         epsilon = 1e-4, maxiter = 100):\n",
    "    \"\"\"Low-Rank Tenor Completion with Truncated Nuclear Norm, LRTC-TNN.\"\"\"\n",
    "    \n",
    "    dim = np.array(sparse_tensor.shape)\n",
    "    pos_missing = np.where(sparse_tensor == 0)\n",
    "    pos_test = np.where((dense_tensor != 0) & (sparse_tensor == 0))\n",
    "    \n",
    "    X = np.zeros((int(len(dim)), int(dim[0]), int(dim[1]), int(dim[2])))\n",
    "    T = np.zeros((int(len(dim)), int(dim[0]), int(dim[1]), int(dim[2]))) \n",
    "    Z = sparse_tensor.copy()\n",
    "    last_tensor = sparse_tensor.copy()\n",
    "    snorm = np.sqrt(np.sum(sparse_tensor ** 2))\n",
    "    it = 0\n",
    "    while True:\n",
    "        rho = min(rho * 1.05, 1e5)\n",
    "        for k in range(len(dim)):\n",
    "            X[k] = mat2ten(svt_tnn(ten2mat(Z - T[k] / rho, k), alpha[k], rho, theta), dim, k)\n",
    "        Z[pos_missing] = np.mean(X + T / rho, axis = 0)[pos_missing]\n",
    "        temp = np.zeros((int(len(dim)), int(dim[0]), int(dim[1]), int(dim[2])))\n",
    "        for i in range(int(len(dim))):\n",
    "            temp[i] = Z\n",
    "        T = T + rho * (X - temp)\n",
    "        tensor_hat = np.einsum('k, kmnt -> mnt', alpha, X)\n",
    "        tol = np.sqrt(np.sum((tensor_hat - last_tensor) ** 2)) / snorm\n",
    "        last_tensor = tensor_hat.copy()\n",
    "        it += 1\n",
    "        if (it + 1) % 100 == 0:\n",
    "            print('Iter: {}'.format(it + 1))\n",
    "            print('MAPE: {:.6}'.format(compute_mape(dense_tensor[pos_test], tensor_hat[pos_test])))\n",
    "            print('RMSE: {:.6}'.format(compute_rmse(dense_tensor[pos_test], tensor_hat[pos_test])))\n",
    "            print()\n",
    "        if (tol < epsilon) or (it >= maxiter):\n",
    "            break\n",
    "\n",
    "    print('Imputation MAPE: {:.6}'.format(compute_mape(dense_tensor[pos_test], tensor_hat[pos_test])))\n",
    "    print('Imputation RMSE: {:.6}'.format(compute_rmse(dense_tensor[pos_test], tensor_hat[pos_test])))\n",
    "    print()\n",
    "    \n",
    "    return tensor_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "dense_mat = np.load('pems-w1.npz')['arr_0']\n",
    "for t in range(2, 5):\n",
    "    dense_mat = np.append(dense_mat, np.load('pems-w{}.npz'.format(t))['arr_0'],\n",
    "                          axis = 1)\n",
    "dim1, dim2 = dense_mat.shape\n",
    "\n",
    "missing_rate = 0.3\n",
    "sparse_mat = dense_mat * np.round(np.random.rand(dim1, dim2) + 0.5 - missing_rate)\n",
    "np.savez_compressed('dense_mat.npz', dense_mat)\n",
    "np.savez_compressed('sparse_mat.npz', sparse_mat)\n",
    "\n",
    "import cupy as np\n",
    "\n",
    "dense_mat = np.load('dense_mat.npz')['arr_0'][:, : 14 * 288]\n",
    "sparse_mat = np.load('sparse_mat.npz')['arr_0'][:, : 14 * 288]\n",
    "dense_tensor = mat2ten(dense_mat, np.array([dim1, 288, 14]), 0)\n",
    "sparse_tensor = mat2ten(sparse_mat, np.array([dim1, 288, 14]), 0)\n",
    "del dense_mat, sparse_mat\n",
    "\n",
    "import time\n",
    "for lmbda in [1e-4, 1e-5]:\n",
    "    for theta in [5, 10, 15, 20, 25]:\n",
    "        print('lmbda = {}'.format(lmbda))\n",
    "        print('theta = {}'.format(theta))\n",
    "        start = time.time()\n",
    "        alpha = np.ones(3) / 3\n",
    "        tensor_hat = LRTC(dense_tensor, sparse_tensor,\n",
    "                          alpha, lmbda, theta)\n",
    "        end = time.time()\n",
    "        print('Running time: %d seconds.'%(end - start))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report imputation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1000)\n",
    "\n",
    "dense_mat = np.load('pems-w1.npz')['arr_0']\n",
    "for t in range(2, 5):\n",
    "    dense_mat = np.append(dense_mat, np.load('pems-w{}.npz'.format(t))['arr_0'],\n",
    "                          axis = 1)\n",
    "dim1, dim2 = dense_mat.shape\n",
    "\n",
    "missing_rate = 0.3\n",
    "sparse_mat = dense_mat * np.round(np.random.rand(dim1, dim2) + 0.5 - missing_rate)\n",
    "np.savez_compressed('dense_mat.npz', dense_mat)\n",
    "np.savez_compressed('sparse_mat.npz', sparse_mat)\n",
    "\n",
    "import cupy as np\n",
    "\n",
    "dense_mat = np.load('dense_mat.npz')['arr_0'][:, 14 * 288 :]\n",
    "sparse_mat = np.load('sparse_mat.npz')['arr_0'][:, 14 * 288 :]\n",
    "dense_tensor = mat2ten(dense_mat, np.array([dim1, 288, 14]), 0)\n",
    "sparse_tensor = mat2ten(sparse_mat, np.array([dim1, 288, 14]), 0)\n",
    "del dense_mat, sparse_mat\n",
    "\n",
    "import time\n",
    "for lmbda in [1e-5]:\n",
    "    for theta in [25]:\n",
    "        print('lmbda = {}'.format(lmbda))\n",
    "        print('theta = {}'.format(theta))\n",
    "        start = time.time()\n",
    "        alpha = np.ones(3) / 3\n",
    "        tensor_hat = LRTC(dense_tensor, sparse_tensor,\n",
    "                          alpha, lmbda, theta)\n",
    "        end = time.time()\n",
    "        print('Running time: %d seconds.'%(end - start))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### License\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>This work is released under the MIT license.</b>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
